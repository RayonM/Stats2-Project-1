---
title: "Project 1 Group Code"
author: "Ely, Kalen, Rayon"
date: ''
output:
  word_document: default
  html_document:
    df_print: paged
fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Libraries
library("tidyverse")
library("ggplot2")
library("GGally")
library("naniar")
library("caret")
library("ggpubr")
library("glmnet")
library(car)

```




```{r}

#Read Data
car_data = read.csv("~/Desktop/applied_stats/data1.csv")

head(car_data)

```


# Cleaning Data

```{r}
# Change categorical attributes to factors

car_data$Make = as.factor(car_data$Make)
car_data$Model = as.factor(car_data$Model)
car_data$Engine.Fuel.Type = as.factor(car_data$Engine.Fuel.Type)
car_data$Transmission.Type = as.factor(car_data$Transmission.Type)
car_data$Driven_Wheels = as.factor(car_data$Driven_Wheels)
car_data$Vehicle.Size = as.factor(car_data$Vehicle.Size)
car_data$Market.Category = as.factor(car_data$Market.Category)
car_data$Vehicle.Style = as.factor(car_data$Vehicle.Style)
car_data$Engine.Cylinders = as.factor(car_data$Engine.Cylinders)


```

```{r}
# Evaluate if there is missing information - NA
gg_miss_var(car_data)



# Visualize the missing data. DOES NOT WORK

#vis_miss(new_AutoM) # Where is new_AutoM?
```


```{r}


# Missing Doors

car_data %>%
  filter(is.na(Number.of.Doors))

# Other Tesla Model S 2016 have 4 doors - manual fix
car_data %>%
  filter(Make == 'Tesla' & Model == 'Model S' & Year == 2016)

car_data[(is.na(car_data$Number.of.Doors)&car_data$Make=='Tesla'),9] = 4

# There are no other 2013 FF Ferraris in our data set, but the other years have 2 doors, and some online research shows thee 2013 does as well - manual fix
car_data %>%
  filter(Make == 'Ferrari' & Model =='FF')

car_data[(is.na(car_data$Number.of.Doors)&car_data$Make=='Ferrari'),9] = 2


```


```{r}

# Engine Cylinders

head(car_data %>%
  filter(is.na(Engine.Cylinders)))


# Mazda RX Models - These have a rotary engine, so have 0 cylinders

car_data[(is.na(car_data$Engine.Cylinders) & car_data$Make == 'Mazda'),6]= 0

# Other missing cylinder cars are all electric

car_data[(is.na(car_data$Engine.Cylinders) & car_data$Engine.Fuel.Type == 'electric'),6]= 0


```

```{r}

# Engine Horsepower

head(car_data %>%
  filter(is.na(Engine.HP)))

#  Using the base horsepower I can find online for these Some versions have higher horsepower, but this data set does not give us the attributes needed to identify these.

car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Model S' & car_data$Year == 2014),5] = 302
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Model S' & car_data$Year == 2015),5] = 329
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Model S' & car_data$ear == 2016),5] = 315
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='500e' & car_data$Year == 2015),5] = 111
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='500e' & car_data$Year == 2016),5] = 111
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='500e' & car_data$Year == 2017),5] = 111
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Continental' & car_data$Year == 2017),5] =305
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Escape' & car_data$Year == 2017),5] = 168
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Fit EV' & car_data$Year == 2013),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Fit EV' & car_data$Year == 2014),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Focus' & car_data$Year == 2015),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Focus' & car_data$Year == 2016),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Focus' & car_data$Year == 2017),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Freestar' & car_data$Year == 2005),5] = 193
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='i-MiEV' & car_data$Year == 2014),5] = 66
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Impala' & car_data$Year == 2015),5] = 195
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Impala' & car_data$Year == 2016),5] = 196
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Impala' & car_data$Year == 2017),5] = 197
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Leaf' & car_data$Year == 2014),5] = 107
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Leaf' & car_data$Year == 2015),5] = 107
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Leaf' & car_data$Year == 2016),5] = 107
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='M-Class' & car_data$Year == 2015),5] = 302
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='MKZ' & car_data$Year == 2017),5] = 240
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='RAV4 EV' & car_data$Year == 2013),5] = 154
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='RAV4 EV' & car_data$Year == 2014),5] = 154
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Soul EV' & car_data$Year == 2015),5] = 109
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Soul EV' & car_data$Year == 2016),5] = 109




# Double Check NA Values - Looks good!
gg_miss_var(car_data)
sapply(car_data, function(x) sum(is.na(x)))

```

```{r}
# Unnown Transmission Types

head(car_data %>%
  filter(Transmission.Type == 'UNKNOWN'))

# Based off of other RAM 150s
car_data[(car_data$Transmission.Type == 'UNKNOWN' & car_data$Model=='RAM 150'),7] = 'MANUAL'

# Based off of online research
car_data[(car_data$Transmission.Type == 'UNKNOWN' & car_data$Model=='Achieva'),7] = 'AUTOMATIC'

# Other 3 models with unknown transmissions don't have enough identifiable info to fix, so we'll drop these 9 rows.

car_data = car_data %>%
  filter(Transmission.Type != 'UNKNOWN')

# Refactor transmission type to exclude UNKNOWN
car_data$Transmission.Type = as.character(car_data$Transmission.Type)
car_data$Transmission.Type = as.factor(car_data$Transmission.Type)


```

```{r}
summary(car_data)


```

```{r}


# Seeing odd values for max HP, Engine Cylinders,MPG, and MSRP

# HP and MSRP looks correct
head(car_data %>%
  filter(Engine.HP > 700))

# Cylinders looks correct based on car and HP
head(car_data %>%
  filter(Engine.Cylinders > 10))

# Highway MPG for this item is false - manual fix based on online research
head(car_data %>%
  filter(highway.MPG > 300))

car_data[car_data$highway.MPG == 354,13] = 32

# All high city mpgs are electric, as expected
head(car_data %>%
  filter(city.mpg > 100))


# N/A Strings in Market Category

# These N/A Values range over a wide variety of car types. We'll choose to keep these values in the data set unless we need to create a model using market category

head(
  car_data %>%
    filter(Market.Category == 'N/A')
    )
```

# Accounting for duplicate entries with differing MSRP values

```{r}


# Collaborated with Braden on this code chunk.

avg_multiple_priced_duplicates <- function(df){
  
  count <- 0
  
  # Create a copy of the dataframe to edit
  new_df <- df
  
  for(index in 1:nrow(df)){
    
    # Grab a dataframe row, which we will use to search for rows that are
    # exactly the same, other than having a different MSRP
    df_row <- df[index,]
    
    # Get a set of rows that are the same as  this one (other than different MSRP)
    filtered_data <- filter_by_example(df=df, row_example=df_row)
    
    # Check how many rows matched df_row
    num_matching_rows <- nrow(filtered_data)
    
    # If more than one (itself) matched... there are duplicates
    if(num_matching_rows > 1){
      
      # Get the rownames for the duplicates
      row_names <- rownames(filtered_data)
      
      # Grab the first row name, we will keep this one (arbitrary) and drop the others
      first_row_name <- row_names[1]
      
      # Calculate the average price for these duplicate rows
      average_price <- mean(filtered_data[,"MSRP"])
      
      # Set the MSRP for the first instance of these duplicates to the average
      new_df[rownames(new_df) == first_row_name, "MSRP"] <- average_price
      
      # Remove the rest of the duplicates
      removal_row_names <- row_names[row_names != first_row_name]
      new_df <- new_df[!(rownames(new_df) %in%removal_row_names),]
      
      # Just for keeping track of how many things we remove, in case of troubleshooting.
      count <- count + 1
      
    }
  }
  
  return(new_df)
}

filter_by_example <- function(df, row_example){
  
  filtered_df <- filter_all_columns(df=df,
                                    Make=row_example$Make, 
                                    Model=row_example$Model, 
                                    Year=row_example$Year,
                                    Engine.Fuel.Type=row_example$Engine.Fuel.Type,
                                    Engine.HP=row_example$Engine.HP,
                                    Engine.Cylinders=row_example$Engine.Cylinders,
                                    Transmission.Type=row_example$Transmission.Type,
                                    Driven_Wheels=row_example$Driven_Wheels,
                                    Number.of.Doors=row_example$Number.of.Doors,
                                    Market.Category=row_example$Market.Category,
                                    Vehicle.Size=row_example$Vehicle.Size,
                                    Vehicle.Style=row_example$Vehicle.Style,
                                    highway.MPG=row_example$highway.MPG,
                                    city.mpg=row_example$city.mpg,
                                    Popularity=row_example$Popularity)
  
  return(filtered_df)
  
}

filter_all_columns <- function(df, Make, Model, Year, Engine.Fuel.Type, Engine.HP, Engine.Cylinders, 
                               Transmission.Type, Driven_Wheels, Number.of.Doors, Market.Category, Vehicle.Size, 
                               Vehicle.Style, highway.MPG, city.mpg, Popularity){
  
  
  f1 <- df[,"Make"] == Make
  f2 <- df[,"Model"] == Model
  f3 <- df[,"Year"] == Year
  f4 <- df[,"Engine.Fuel.Type"] == Engine.Fuel.Type
  f5 <- df[,"Engine.HP"] == Engine.HP
  f6 <- df[,"Engine.Cylinders"] == Engine.Cylinders
  f7 <- df[,"Transmission.Type"] == Transmission.Type
  f8 <- df[,"Driven_Wheels"] == Driven_Wheels
  f9 <- df[,"Number.of.Doors"] == Number.of.Doors
  f10 <- df[,"Market.Category"] == Market.Category
  f11 <- df[,"Vehicle.Size"] == Vehicle.Size
  f12 <- df[,"Vehicle.Style"] == Vehicle.Style
  f13 <- df[,"highway.MPG"] == highway.MPG
  f14 <- df[,"city.mpg"] == city.mpg
  f15 <- df[,"Popularity"] == Popularity
  
  full_filter <- (f1& f2& f3 & f4 & f5 & f6 & f7 & f8 & f9 & f10 & f11 & f12 & f13 & f14 & f15)
  
  filtered_df <- df[full_filter,]
  
  return(filtered_df)
  
}


car_data1 <- avg_multiple_priced_duplicates(df=car_data)


```

# Removing Electric Vehicles and outlier MSRP values

```{r}
 car_data_clean = car_data1 %>%
  filter(Engine.Fuel.Type != 'electric') %>%
  filter(MSRP > 10000) %>%
  filter(MSRP < 150000)
```

# Transforming Response and Predictor Variables

```{r}
car_data_clean = 
  car_data_clean %>%
  mutate(logYear = log(Year)) %>%
  mutate(logEngineHP = log(Engine.HP)) %>%
  mutate(logMSRP = log(MSRP)) %>%
  mutate(loghighwayMPG = log(highway.MPG)) %>%
  mutate(logcitympg = log(city.mpg)) %>%
  mutate(sqrtMSRP=sqrt(MSRP)) %>% 
  mutate(invertedMSRP=1/MSRP) %>% 
  mutate(Observation = n()) # Created to merge the data set to find leverage points
  
```


# Training / Test Split

```{r}

rows = count(car_data_clean)[1]
train_rows = as.numeric(round(0.8*rows,0))
test_validate_rows = as.numeric((rows - train_rows)/2)

set.seed(1234)
index = sample(1:dim(car_data_clean)[1],train_rows,replace=F)

train = car_data_clean[index,]
test_validate = car_data_clean[-index,]
test_index = sample(1:dim(test_validate)[1],test_validate_rows,replace=F)

train = car_data_clean[index,]
test = test_validate[test_index,]
validate = test_validate[-test_index,]

head(train)
head(test)
head(validate)
```


# EDA


```{r}

head(train)
pairs(train[,c(16,3,5,6,9,13,14,15)])

```


**Initial Findings**

  * City and highway mpg are highly correlated.
 
  * Seeing some correlation between HP and Cylinders as well
 
```{r}
pairs(train[,c(15,5,6,13,14)])

# City vs Highway MPG
train %>%
  ggplot(aes(x = highway.MPG, y = city.mpg)) +
  geom_point() +
  geom_smooth(method = "lm") +
  xlab("Highway MPG") +
  ylab("City MPG") +
  ggtitle("City MPG vs Highway MPG")

# Horsepower vs Cylinders
train %>%
  ggplot(aes(x = Engine.Cylinders, y = Engine.HP)) +
  geom_boxplot() +
  xlab("Cylinders") +
  ylab("Horsepower") +
  ggtitle("Cylinders vs Horsepower")

```

**Findings**
 
  * We'll need to choose between HP vs Cylinders and Highway mpg vs City mpg
 
```{r}
pairs(train[,c(16,5,6)])

pairs(train[,c(16,13,14)])

# highway mpg is statistically significant when used as a predictor for popularity (did you mean MSRP?) while city mpg is not. We'll go with highway mpg
summary(lm(MSRP ~ highway.MPG, data = train))[4]
summary(lm(MSRP ~ city.mpg, data = train))[4]

# Checking against Log MSRP
pairs(train[,c(19,13,14)])

summary(lm(logMSRP ~ highway.MPG, data = train))[4]
summary(lm(logMSRP ~ city.mpg, data = train))[4]

# Will stick with highway mpg


# Both are statistically significant - next we'll test to see how they hold up when paired with highway mpg


summary(lm(MSRP ~ Engine.HP, data = train))[4]
summary(lm(MSRP ~ Engine.Cylinders, data = train))[4]

# Checking against Log MSRP
pairs(train[,c(19,5,6)])

summary(lm(logMSRP ~ Engine.HP, data = train))[4]
summary(lm(logMSRP ~ Engine.Cylinders, data = train))[4]

pairs(train[,c(16,5,6,13)])
summary(lm(Popularity ~ Engine.HP + highway.MPG, data = train))[4]
summary(lm(Popularity ~ Engine.Cylinders + highway.MPG, data = train))[4]

# Seeing some collinearity between mpg, horsepower, and cylinders. We'll move forward with caution using highway mpg and cylinders, understanding that we may ultimately need to decide between the two.

head(train)

#Numerical
pairs(train[,c(16,6,9,13,15)])

# Numerical with log msrp
pairs(train[,c(19,9,13,15)]) # Log MSRP tends to show a more linear relationship



#Seeing some correlation with MSRP vs Year
pairs(train[,c(19,1,3,4)])

pairs(train[,c(19,7:11)])
```

# Engine Horsepower and MSRP Comparisons

```{r}
train %>%
  ggplot(aes(x = Engine.HP, y = MSRP)) +
  geom_point() +
  ylab("MSRP") +
  xlab("Horsepower") +
  ggtitle("MSRP vs Horsepower")

train %>%
  ggplot(aes(x = Engine.HP, y = logMSRP)) +
  geom_point() +
  ylab("Log MSRP") +
  xlab("Horsepower") +
  ggtitle("Log MSRP vs Horsepower")

train %>%
  ggplot(aes(x = logEngineHP, y = logMSRP)) +
  geom_point() +
  ylab("Log MSRP") +
  xlab("Log Horsepower") +
  ggtitle("MSRP vs Horsepower")
```

# Potential Predictors

```{r}

attributes = subset(train, select = c("Year","Transmission.Type","Driven_Wheels","Engine.HP","Number.of.Doors","Vehicle.Size","highway.MPG","Popularity","logMSRP"))

pairs(attributes)



p1 = attributes %>%
  ggplot(aes(x = reorder(Driven_Wheels, logMSRP), y = logMSRP)) +
  geom_boxplot() +
  ylab("Log MRSP") +
  xlab("Number of Wheels") +
  ggtitle("Number of Wheels vs Log MSRP")

p2 = attributes %>%
  ggplot(aes(x = Year, y = logMSRP)) +
  geom_point(position = "jitter") +
  geom_smooth(method = "lm") +
  ylab("Log MRSP") +
  xlab("Year") +
  ggtitle("Year vs Log MSRP")


p3 = attributes %>%
  ggplot(aes(x = reorder(Transmission.Type, logMSRP), y = logMSRP)) +
  geom_boxplot() +
  ylab("Log MRSP") +
  xlab("Transmission Type") +
  ggtitle("Transmission Type vs Log MSRP")

p4 = attributes %>%
  ggplot(aes(x = reorder(Vehicle.Size, logMSRP), y = logMSRP)) +
  geom_boxplot() +
  ylab("Log MRSP") +
  xlab("Vehicle Size") +
  ggtitle("Vehicle Size vs Log MSRP")

ggarrange(p1,p2,p3,p4)

```


## Importance of Popularity:

#### Popularity is broken down by make - not by model. Also, no two makes have the same popularity. This means that popularity has a 1-1 relationship with Make, so we only need one of them

```{r}
car_data %>%
  select(Make, Model, Popularity) %>%
  group_by(Make) %>%
  summarize(count = n(), max = max(Popularity), min = min(Popularity)) %>%
  arrange(max)
```

#### Popularity vs MSRP

```{r}
train %>%
  ggplot(aes(x = Popularity, y = MSRP)) +
  geom_point() +
  geom_smooth(method = 'lm', color = 'red')

summary(lm(MSRP~Popularity, data = train))
```

<br>

**Findings**

  * There exists a negative correlation between popularity score and MSRP
   
    * This may indicate that more affordable cars are more popular
   
  * The relationship between popularity and MRSP is statistically significant
  


  
# Model Creation Work


## Investigation of potential predictors noted in EDA

```{r}
eda_attributes = subset(train, select = c("Year","Transmission.Type","Driven_Wheels","Engine.HP","Vehicle.Size","highway.MPG","Popularity","logMSRP"))


full.model<-lm(logMSRP~.,data=eda_attributes)
vif(full.model)[,3]^2
```

**Findings**

We're not seeing much evidence of collinearity between these attributes, which is a good sign. We'll likely include these in our final model. Other attributes will be found through variable selection techniques.


# Lasso Variable Selectioin

```{r}

x=model.matrix(logMSRP~.,train[,c(3,4,6:9,11,13,15,19)])[,-1]
y=train$MSRP

xtest=model.matrix(logMSRP~.,test[,c(3,4,6:9,11,13,15,19)])[,-1]
ytest=test$MSRP

grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)
bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
lasso.pred=predict (lasso.mod ,s=bestlambda ,newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO


coef(lasso.mod,s=bestlambda)
```


# LASSO Model - to be used as a reference for final model selection

```{r}
model = lm(logMSRP~ Engine.Fuel.Type + Engine.Cylinders+Transmission.Type+Driven_Wheels+Number.of.Doors+Vehicle.Size+highway.MPG+Popularity, data = train)

summary(model)

par(mfrow=c(2,2))
plot(model)
```


# Final Model

```{r}


final_model = lm(logMSRP~Year+Transmission.Type+Driven_Wheels+Engine.HP+Number.of.Doors+Vehicle.Size+highway.MPG+Popularity, data = train)
summary(final_model)
confint(final_model)
par(mfrow=c(2,2))
plot(final_model)

summary(train)
```

# Test Final Model Against Test Data

```{r}
data_mod = data.frame(Predicted = predict(final_model,test), Observed = test$logMSRP)

data_mod %>%
  ggplot(aes(x = Predicted, y = Observed)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = 'red', size = 2) +
  ggtitle("Predictions vs Actual Values")


```


# Part 2

## Complex model creation:

```{r}
complex_model = lm(logMSRP~Year+Transmission.Type+Driven_Wheels + +logEngineHP+Number.of.Doors+Vehicle.Size+highway.MPG+Popularity+ I(Number.of.Doors^2) + logEngineHP*highway.MPG + Year*highway.MPG + Year*Popularity, data = train)

plot(complex_model)
summary(complex_model)

data_mod = data.frame(Predicted = predict(complex_model,test), Observed = test$logMSRP)

data_mod %>%
  ggplot(aes(x = Predicted, y = Observed)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = 'red', size = 2) +
  ggtitle("Predictions vs Actual Values")
```



```{r}

set.seed(1)

colSums(is.na(train))

colSums(is.na(test))

head(train)

colSums(is.na(validate))
train_ = subset(train, select = c("Engine.HP", "Number.of.Doors", "city.mpg", "Popularity", "logMSRP"))
train_$city.mpg = log(train_$city.mpg)
train_MSRP = train_$logMSRP
test_ = subset(test, select = c("Engine.HP", "Number.of.Doors", "city.mpg", "Popularity", "logMSRP"))
test_$city.mpg = log(test_$city.mpg)
test_$city.mpg = log(test_$city.mpg)
test_MSRP = test_$logMSRP
validation_ = subset(validate, select = c("Engine.HP", "Number.of.Doors", "city.mpg", "Popularity", "logMSRP"))
validation_$city.mpg = log(validation_$city.mpg)
validation_$MSRP = validation_$logMSRP

```

##KNN Model

```{r}
fitControl = trainControl(method = "repeatedcv", number = 10, repeats = 10)

knn_model = train(logMSRP~., data = train_, method = "knn", preProcess = c("center", "scale"), trControl = fitControl, tuneGrid = data.frame(k = c(1:10, 15, 20, 25, 30)))


plot(knn_model, main = "RMSE per Neighbors of KNN Regression model")

```
 # Testing Knn Model against test and validation sets

```{r}
knn_pred_test = predict(knn_model, test_)

test_ase_knn = mean(train_$logMSRP - knn_pred_test)^2

test_ase_knn


knn_pred_validation = predict(knn_model, validation_)

#Error metrics
knn_validate = postResample(pred = knn_pred_validation, obs = validation_$MSRP)

validation_ase_knn = mean(train_$MSRP - knn_pred_validation)^2

validation_ase_knn

plot(knn_pred_validation, validation_$MSRP, main = "Predicted vs Actual")
lines(0:2000, 0:2000)

varImp(knn_model)
plot(varImp(knn_model), main = "Importance of Variables for the KNN Model")

```



# Regression Tree


```{r}

fitControl = trainControl(method = "repeatedcv", number = 10, repeats = 10)
reg_tree_model = train(logMSRP~., data = train_, method = "rpart", minsplit = 5, trControl = fitControl,
                       tuneGrid = data.frame(cp=c(.005,.0008,.01,.015,.02,.025,.03,.035,.04,.05,.06,.07,.08,.09,.25,.4)))



plot(reg_tree_model, main = "RMSE per Complexity Parameter of Regression Tree Model")


```

# Testing Regression Tree model against test and validation sets

```{r}

test_ase_ref_tree = mean(train_$logMSRP - predict(reg_tree_model, test_))^2


test_ase_ref_tree


plot(reg_tree_model$finalModel, main = "Final Regression Tree Model")

reg_tree_pred = predict(reg_tree_model, validation_)

reg_tree_validate = postResample(pred = reg_tree_pred, obs = validation_$logMSRP)
reg_tree_validate

validation_ase_reg_tree = mean(train_$logMSRP - predict(reg_tree_model, validation_))^2

validation_ase_reg_tree

plot(reg_tree_pred, validation_$logMSRP, main = "Predicted vs Actual")
lines(0:2000, 0:2000)

varImp(reg_tree_model)

plot(varImp(reg_tree_model), main = "Importance of Variables of Regression Tree Model")
```

# Model Comparisons

```{r}
# Final Model

test_ase_final_model = mean(train$logMSRP - predict(final_model, test))^2

final_model_pred = predict(final_model, validate)
final_model_validate = postResample(pred = final_model_pred, obs = validate$logMSRP)

validation_ase_final_model = mean(train$logMSRP - predict(final_model, validate))^2

test_ase_final_model
validation_ase_final_model
final_model_validate




# Complex Model

test_ase_complex_model = mean(train$logMSRP - predict(complex_model, test))^2

complex_model_pred = predict(complex_model, validate)
complex_model_validate = postResample(pred = complex_model_pred, obs = validate$logMSRP)

validation_ase_complex_model = mean(train$logMSRP - predict(complex_model, validate))^2

test_ase_complex_model
validation_ase_complex_model
complex_model_validate


# KNN Model

test_ase_knn = mean(train_$logMSRP - predict(reg_tree_model, test_))^2

knn_pred = predict(knn_model, validation_)
knn_validate = postResample(pred = knn_pred, obs = validation_$logMSRP)

validation_ase_knn = mean(train_$logMSRP - predict(knn_model, validation_))^2

test_ase_knn
validation_ase_knn
knn_validate


#Regression Tree Model

test_ase_ref_tree = mean(train_$logMSRP - predict(reg_tree_model, test_))^2

reg_tree_pred = predict(reg_tree_model, validation_)
reg_tree_validate = postResample(pred = reg_tree_pred, obs = validation_$logMSRP)

validation_ase_reg_tree = mean(train_$logMSRP - predict(reg_tree_model, validation_))^2

test_ase_ref_tree
validation_ase_reg_tree
reg_tree_validate


```
