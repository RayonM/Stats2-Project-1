---
title: "Project 1 Group Code"
author: "Ely, Kalen, Rayon"
date: "2/1/2022"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Libraries
library("tidyverse")
library("ggplot2")
library("GGally")
library("naniar")

```




```{r}
# Read Data <- This will be different for each of us.

#Ely's read data line
car_data = read.csv("~/Desktop/applied_stats/data1.csv")

# Rayon's read data line
#car_data <-  read.csv("data1.csv")
#attach(car_data)

```


# Cleaning Data

```{r}
# Change categorical attributes to factors

car_data$Make = as.factor(car_data$Make)
car_data$Model = as.factor(car_data$Model)
car_data$Engine.Fuel.Type = as.factor(car_data$Engine.Fuel.Type)
car_data$Transmission.Type = as.factor(car_data$Transmission.Type)
car_data$Driven_Wheels = as.factor(car_data$Driven_Wheels)
car_data$Vehicle.Size = as.factor(car_data$Vehicle.Size)
car_data$Market.Category = as.factor(car_data$Market.Category)
car_data$Vehicle.Style = as.factor(car_data$Vehicle.Style)
car_data$Engine.Cylinders = as.factor(car_data$Engine.Cylinders)


```

```{r}
# Evaluate if there is missing information - NA
gg_miss_var(car_data)



# Visualize the missing data. DOES NOT WORK

#vis_miss(new_AutoM) # Where is new_AutoM?
```


```{r}


# Missing Doors

car_data %>%
  filter(is.na(Number.of.Doors))

# Other Tesla Model S 2016 have 4 doors - manual fix
car_data %>%
  filter(Make == 'Tesla' & Model == 'Model S' & Year == 2016)

car_data[(is.na(car_data$Number.of.Doors)&car_data$Make=='Tesla'),9] = 4

# There are no other 2013 FF Ferraris in our data set, but the other years have 2 doors, and some online research shows thee 2013 does as well - manual fix
car_data %>%
  filter(Make == 'Ferrari' & Model =='FF')

car_data[(is.na(car_data$Number.of.Doors)&car_data$Make=='Ferrari'),9] = 2


```


```{r}

# Engine Cylinders

car_data %>%
  filter(is.na(Engine.Cylinders))


# Mazda RX Models - These have a rotary engine, so have 0 cylinders

car_data[(is.na(car_data$Engine.Cylinders) & car_data$Make == 'Mazda'),6]= 0

# Other missing cylinder cars are all electric

car_data[(is.na(car_data$Engine.Cylinders) & car_data$Engine.Fuel.Type == 'electric'),6]= 0


```

```{r}

# Engine Horsepower

car_data %>%
  filter(is.na(Engine.HP))

#  Using the base horsepower I can find online for these Some versions have higher horsepower, but this data set does not give us the attributes needed to identify these.

car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Model S' & Year == 2014),5] = 302
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Model S' & Year == 2015),5] = 329
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Model S' & Year == 2016),5] = 315
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='500e' & Year == 2015),5] = 111
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='500e' & Year == 2016),5] = 111
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='500e' & Year == 2017),5] = 111
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Continental' & Year == 2017),5] =305
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Escape' & Year == 2017),5] = 168
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Fit EV' & Year == 2013),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Fit EV' & Year == 2014),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Focus' & Year == 2015),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Focus' & Year == 2016),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Focus' & Year == 2017),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Freestar' & Year == 2005),5] = 193
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='i-MiEV' & Year == 2014),5] = 66
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Impala' & Year == 2015),5] = 195
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Impala' & Year == 2016),5] = 196
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Impala' & Year == 2017),5] = 197
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Leaf' & Year == 2014),5] = 107
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Leaf' & Year == 2015),5] = 107
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Leaf' & Year == 2016),5] = 107
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='M-Class' & Year == 2015),5] = 302
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='MKZ' & Year == 2017),5] = 240
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='RAV4 EV' & Year == 2013),5] = 154
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='RAV4 EV' & Year == 2014),5] = 154
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Soul EV' & Year == 2015),5] = 109
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Soul EV' & Year == 2016),5] = 109




# Double Check NA Values - Looks good!
gg_miss_var(car_data)
sapply(car_data, function(x) sum(is.na(x)))

```

```{r}
# Unnown Transmission Types

car_data %>%
  filter(Transmission.Type == 'UNKNOWN')

# Based off of other RAM 150s
car_data[(car_data$Transmission.Type == 'UNKNOWN' & car_data$Model=='RAM 150'),7] = 'MANUAL'

# Based off of online research
car_data[(car_data$Transmission.Type == 'UNKNOWN' & car_data$Model=='Achieva'),7] = 'AUTOMATIC'

# Other 3 models with unknown transmissions don't have enough identifiable info to fix, so we'll drop these 9 rows.

car_data = car_data %>%
  filter(Transmission.Type != 'UNKNOWN')

# Refactor transmission type to exclude UNKNOWN
car_data$Transmission.Type = as.character(car_data$Transmission.Type)
car_data$Transmission.Type = as.factor(car_data$Transmission.Type)


```

```{r}
summary(car_data)


```

```{r}


# Seeing odd values for max HP, Engine Cylinders,MPG, and MSRP

# HP and MSRP looks correct
car_data %>%
  filter(Engine.HP > 700)

# Cylinders looks correct based on car and HP
car_data %>%
  filter(Engine.Cylinders > 10)

# Highway MPG for this item is false - manual fix based on online research
car_data %>%
  filter(highway.MPG > 300)

car_data[car_data$highway.MPG == 354,13] = 32

# All high city mpgs are electric, as expected
car_data %>%
  filter(city.mpg > 100)


# N/A Strings in Market Category

# These N/A Values range over a wide variety of car types. We'll choose to keep these values in the data set unless we need to create a model using market category

head(
  car_data %>%
    filter(Market.Category == 'N/A')
    )
```



```{r}
# Added this section for Further data cleaning.
# Thoughts:
#   1. We should remove all vehicles above $500,000. The data set of car beyond that point is very small. 
#   2. We should eliminate duplicates. The data is filled with a lot of duplicate records.
#   3. After this, we'll be left with what looks like duplicates but they won't. the records remaining will have some matches except on the MSRP column. We should average those and keep one with the mean MSRP.

# Collaborated with Braden on this code chunk.

avg_multiple_priced_duplicates <- function(df){
  
  count <- 0
  
  # Create a copy of the dataframe to edit
  new_df <- df
  
  for(index in 1:nrow(df)){
    
    # Grab a dataframe row, which we will use to search for rows that are
    # exactly the same, other than having a different MSRP
    df_row <- df[index,]
    
    # Get a set of rows that are the same as  this one (other than different MSRP)
    filtered_data <- filter_by_example(df=df, row_example=df_row)
    
    # Check how many rows matched df_row
    num_matching_rows <- nrow(filtered_data)
    
    # If more than one (itself) matched... there are duplicates
    if(num_matching_rows > 1){
      
      # Get the rownames for the duplicates
      row_names <- rownames(filtered_data)
      
      # Grab the first row name, we will keep this one (arbitrary) and drop the others
      first_row_name <- row_names[1]
      
      # Calculate the average price for these duplicate rows
      average_price <- mean(filtered_data[,"MSRP"])
      
      # Set the MSRP for the first instance of these duplicates to the average
      new_df[rownames(new_df) == first_row_name, "MSRP"] <- average_price
      
      # Remove the rest of the duplicates
      removal_row_names <- row_names[row_names != first_row_name]
      new_df <- new_df[!(rownames(new_df) %in%removal_row_names),]
      
      # Just for keeping track of how many things we remove, in case of troubleshooting.
      count <- count + 1
      
    }
  }
  
  return(new_df)
}

filter_by_example <- function(df, row_example){
  
  filtered_df <- filter_all_columns(df=df,
                                    Make=row_example$Make, 
                                    Model=row_example$Model, 
                                    Year=row_example$Year,
                                    Engine.Fuel.Type=row_example$Engine.Fuel.Type,
                                    Engine.HP=row_example$Engine.HP,
                                    Engine.Cylinders=row_example$Engine.Cylinders,
                                    Transmission.Type=row_example$Transmission.Type,
                                    Driven_Wheels=row_example$Driven_Wheels,
                                    Number.of.Doors=row_example$Number.of.Doors,
                                    Market.Category=row_example$Market.Category,
                                    Vehicle.Size=row_example$Vehicle.Size,
                                    Vehicle.Style=row_example$Vehicle.Style,
                                    highway.MPG=row_example$highway.MPG,
                                    city.mpg=row_example$city.mpg,
                                    Popularity=row_example$Popularity)
  
  return(filtered_df)
  
}

filter_all_columns <- function(df, Make, Model, Year, Engine.Fuel.Type, Engine.HP, Engine.Cylinders, 
                               Transmission.Type, Driven_Wheels, Number.of.Doors, Market.Category, Vehicle.Size, 
                               Vehicle.Style, highway.MPG, city.mpg, Popularity){
  
  
  f1 <- df[,"Make"] == Make
  f2 <- df[,"Model"] == Model
  f3 <- df[,"Year"] == Year
  f4 <- df[,"Engine.Fuel.Type"] == Engine.Fuel.Type
  f5 <- df[,"Engine.HP"] == Engine.HP
  f6 <- df[,"Engine.Cylinders"] == Engine.Cylinders
  f7 <- df[,"Transmission.Type"] == Transmission.Type
  f8 <- df[,"Driven_Wheels"] == Driven_Wheels
  f9 <- df[,"Number.of.Doors"] == Number.of.Doors
  f10 <- df[,"Market.Category"] == Market.Category
  f11 <- df[,"Vehicle.Size"] == Vehicle.Size
  f12 <- df[,"Vehicle.Style"] == Vehicle.Style
  f13 <- df[,"highway.MPG"] == highway.MPG
  f14 <- df[,"city.mpg"] == city.mpg
  f15 <- df[,"Popularity"] == Popularity
  
  full_filter <- (f1& f2& f3 & f4 & f5 & f6 & f7 & f8 & f9 & f10 & f11 & f12 & f13 & f14 & f15)
  
  filtered_df <- df[full_filter,]
  
  return(filtered_df)
  
}


car_data1 <- avg_multiple_priced_duplicates(df=car_data)


```

# Removing Electric Vehicles and outlier MSRP values

```{r}
 car_data_clean = car_data1 %>%
  filter(Engine.Fuel.Type != 'electric') %>%
  filter(MSRP > 2000) %>%
  filter(MSRP < 500000)
```

# Transforming Response and Predictor Variables

```{r}
car_data_clean = 
  car_data_clean %>%
  mutate(logYear = log(Year)) %>%
  mutate(logEngineHP = log(Engine.HP)) %>%
  mutate(logMSRP = log(MSRP)) %>%
  mutate(loghighwayMPG = log(highway.MPG)) %>%
  mutate(logcitympg = log(city.mpg)) %>%
  mutate(sqrtMSRP=sqrt(MSRP)) %>% 
  mutate(invertedMSRP=1/MSRP) %>% 
  mutate(Observation = n()) # Created to merge the data set to find leverage points
  
```


# Training / Test Split

```{r}

rows = count(car_data_clean)[1]
train_rows = as.numeric(round(0.8*rows,0))
test_validate_rows = as.numeric((rows - train_rows)/2)

set.seed(1234)
index = sample(1:dim(car_data_clean)[1],train_rows,replace=F)

train = car_data_clean[index,]
test_validate = car_data_clean[-index,]
test_index = sample(1:dim(test_validate)[1],test_validate_rows,replace=F)

train = car_data_clean[index,]
test = test_validate[test_index,]
validate = test_validate[-test_index,]

head(train)
head(test)
head(validate)
```


# EDA


```{r}

head(train)
pairs(train[,c(16,3,5,6,9,13,14,15)])

```


**Initial Findings**

  * City and highway mpg are highly correlated.
  * Rayon - I agree with this. 
 
  * Seeing some correlation between HP, Cylinders, and MPG (highway and city) as well
  * Rayon - I agree with this.
 
```{r}
pairs(train[,c(15,5,6,13,14)])
```

**Findings**
 
  * We'll need to choose between HP vs Cylinders and Highway mpg vs City mpg
 
```{r}
pairs(train[,c(16,5,6)])

pairs(train[,c(16,13,14)])

# highway mpg is statistically significant when used as a predictor for popularity (did you mean MSRP?) while city mpg is not. We'll go with highway mpg
summary(lm(MSRP ~ highway.MPG, data = train))[4]
summary(lm(MSRP ~ city.mpg, data = train))[4]

# Checking against Log MSRP
pairs(train[,c(19,13,14)])

summary(lm(logMSRP ~ highway.MPG, data = train))[4]
summary(lm(logMSRP ~ city.mpg, data = train))[4]

# Will stick with highway mpg


# Both are statistically significant - next we'll test to see how they hold up when paired with highway mpg


summary(lm(MSRP ~ Engine.HP, data = train))[4]
summary(lm(MSRP ~ Engine.Cylinders, data = train))[4]

# Checking against Log MSRP
pairs(train[,c(19,5,6)])

summary(lm(logMSRP ~ Engine.HP, data = train))[4]
summary(lm(logMSRP ~ Engine.Cylinders, data = train))[4]

pairs(train[,c(16,5,6,13)])
summary(lm(Popularity ~ Engine.HP + highway.MPG, data = train))[4]
summary(lm(Popularity ~ Engine.Cylinders + highway.MPG, data = train))[4]

# Seeing some collinearity between mpg, horsepower, and cylinders. We'll move forward with caution using highway mpg and cylinders, understanding that we may ultimately need to decide between the two.

head(train)

#Numerical
pairs(train[,c(16,6,9,13,15)])

# Numerical with log msrp
pairs(train[,c(19,9,13,15)]) # Log MSRP tends to show a more linear relationship



#Seeing some correlation with MSRP vs Year
pairs(train[,c(19,1,3,4)])

pairs(train[,c(19,7:11)])
```

# Engine Horsepower and MSRP Comparisons

```{r}
train %>%
  ggplot(aes(x = Engine.HP, y = MSRP)) +
  geom_point() +
  ylab("MSRP") +
  xlab("Horsepower") +
  ggtitle("MSRP vs Horsepower")

train %>%
  ggplot(aes(x = Engine.HP, y = logMSRP)) +
  geom_point() +
  ylab("Log MSRP") +
  xlab("Horsepower") +
  ggtitle("Log MSRP vs Horsepower")

train %>%
  ggplot(aes(x = logEngineHP, y = logMSRP)) +
  geom_point() +
  ylab("Log MSRP") +
  xlab("Log Horsepower") +
  ggtitle("MSRP vs Horsepower")
```
## Importance of Popularity:

#### Popularity is broken down by make - not by model. Also, no two makes have the same popularity. This means that popularity has a 1-1 relationship with Make, so we only need one of them

```{r}
car_data %>%
  select(Make, Model, Popularity) %>%
  group_by(Make) %>%
  summarize(count = n(), max = max(Popularity), min = min(Popularity)) %>%
  arrange(max)
```

#### Popularity vs MSRP

```{r}

# Seeing a slight negative trend between popularity score and MSRP
train %>%
  ggplot(aes(x = Popularity, y = MSRP)) +
  geom_point() +
  geom_smooth(method = 'lm', color = 'red')

summary(lm(MSRP~Popularity, data = train))
```

<br>

**Findings**

  * There exists a negative correlation between popularity score and MSRP
   
    * This may indicate that more affordable cars are more popular
   
  * The relationship between popularity and MRSP is statistically significant
  


  
# Model Creation Work


## Choosing potential predictors
```{r}

# Checking for multicolinnearity - Not including Make, since it is dependent on popularity

head(train[,c(3,4,6,7,8,9,10,11,13,19)])

library(car)

# All VIF is under 10 (though highway MPG is close), so we'll move forward with caution
model_attributes = train[,c(3,4,6:11,13,15,19)]
full.model<-lm(logMSRP~.,data=model_attributes)
vif(full.model)[,3]^2
```
# Should we removing NA from Market Categories? Probably not.
```{r}
train %>%
  filter(Market.Category != 'N/A')
test %>%
  filter(Market.Category != 'N/A')
validate %>%
  filter(Market.Category != 'N/A')

# Removing them takes out A LOT of rows. Maybe best to just not include this attribute...
```

# Predict.Regsubsets fxn

```{r}
predict.regsubsets =function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
```

# Variable Selection (Not including market categories)

## Ely Note: This seems wild. I'm clearly doing something wrong here

```{r}
library(leaps)
reg.fwd = regsubsets(logMSRP~.,data=train[,c(3,4,6:9,11,13,15,19)],method="forward",nvmax=8)

data


testASE<-c()

for (i in 1:8){
  predictions<-predict.regsubsets(object=reg.fwd,newdata=test,id=i) 
  testASE[i]<-mean((test$MSRP-predictions)^2)
}

par(mfrow=c(1,1))
plot(1:8,testASE,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(0,3000000000))
index<-which(testASE==min(testASE))
points(index,testASE[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss
lines(1:8,rss/9525,lty=3,col="blue")   #Dividing by 192 since ASE=RSS/sample size NEED TO ADJUST SAMPLE SIZE DIVISION!!!!
```



# Lasso Attempt

```{r}
library(glmnet)
#Formatting data for GLM net
x=model.matrix(logMSRP~.,train[,c(3,4,6:9,11,13,15,19)])[,-1]
y=train$MSRP

xtest=model.matrix(logMSRP~.,test[,c(3,4,6:9,11,13,15,19)])[,-1]
ytest=test$MSRP

grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)
bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
lasso.pred=predict (lasso.mod ,s=bestlambda ,newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO


coef(lasso.mod,s=bestlambda)
```


# Here's the model LASSO picked - obviously some major collinearities here. Need to work through!
```{r}
model = lm(logMSRP~ Engine.Fuel.Type + Engine.Cylinders+Transmission.Type+Driven_Wheels+Number.of.Doors+Vehicle.Size+highway.MPG+Popularity, data = train)

summary(model)

par(mfrow=c(2,2))
plot(model)


# Slight adjustment - removed obvious collinearities. Still need work

model = lm(MSRP~Model + Year + Engine.Fuel.Type + Engine.Cylinders+Transmission.Type+Driven_Wheels+Number.of.Doors+Market.Category+Vehicle.Style+highway.MPG+Popularity, data = train)

summary(model)

par(mfrow=c(2,2))
plot(model)


```

### This model still is not great - I wonder if we should adjust our response variable ...
### Seeing evidence of violations of normality and equal variance



```{r}
# Logged the response variable but seeing a sharp band in teh Residuals. The qqplot looks a lot better. However, we can see that there are some outliers in the data. Maybe we should some more cleaning. Left a comment up above.


logmodel = lm(logMSRP~Year+Transmission.Type+Driven_Wheels+Number.of.Doors+Vehicle.Size+Popularity, data = train)
summary(logmodel)
par(mfrow=c(2,2))
plot(logmodel)


```

# Final model suggestion
```{r}

# This is very similar to the above model, but I included horsepower and highway.MPG

final_model = lm(logMSRP~Year+Transmission.Type+Driven_Wheels+Engine.HP+Number.of.Doors+Vehicle.Size+highway.MPG+Popularity, data = train)
summary(final_model)
par(mfrow=c(2,2))
plot(final_model)


```






