---
title: "Project 1 Group Code"
author: "Ely, Kalen, Rayon"
date: "2/1/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Libraries
library("tidyverse")
library("ggplot2")
library("GGally")
library("naniar")

```




```{r}
# Read Data <- This will be different for each of us.

#Ely's read data line
#car_data = read.csv("~/Desktop/applied_stats/data1.csv")

# Rayon's read data line
car_data <-  read.csv("data1.csv")
attach(car_data)

```


# Cleaning Data

```{r}
# Change categorical attributes to factors

car_data$Make = as.factor(car_data$Make)
car_data$Model = as.factor(car_data$Model)
car_data$Engine.Fuel.Type = as.factor(car_data$Engine.Fuel.Type)
car_data$Transmission.Type = as.factor(car_data$Transmission.Type)
car_data$Driven_Wheels = as.factor(car_data$Driven_Wheels)
car_data$Vehicle.Size = as.factor(car_data$Vehicle.Size)
car_data$Market.Category = as.factor(car_data$Market.Category)
car_data$Vehicle.Style = as.factor(car_data$Vehicle.Style)
car_data$Engine.Cylinders = as.factor(car_data$Engine.Cylinders)


```

```{r}
# Evaluate if there is missing information - NA
gg_miss_var(car_data)



# Visualize the missing data. DOES NOT WORK

vis_miss(new_AutoM) # Where is new_AutoM?
```


```{r}


# Missing Doors

car_data %>%
  filter(is.na(Number.of.Doors))

# Other Tesla Model S 2016 have 4 doors - manual fix
car_data %>%
  filter(Make == 'Tesla' & Model == 'Model S' & Year == 2016)

car_data[(is.na(car_data$Number.of.Doors)&car_data$Make=='Tesla'),9] = 4

# There are no other 2013 FF Ferraris in our data set, but the other years have 2 doors, and some online research shows thee 2013 does as well - manual fix
car_data %>%
  filter(Make == 'Ferrari' & Model =='FF')

car_data[(is.na(car_data$Number.of.Doors)&car_data$Make=='Ferrari'),9] = 2


```


```{r}

# Engine Cylinders

car_data %>%
  filter(is.na(Engine.Cylinders))


# Mazda RX Models - These have a rotary engine, so have 0 cylinders

car_data[(is.na(car_data$Engine.Cylinders) & car_data$Make == 'Mazda'),6]= 0

# Other missing cylinder cars are all electric

car_data[(is.na(car_data$Engine.Cylinders) & car_data$Engine.Fuel.Type == 'electric'),6]= 0


```

```{r}

# Engine Horsepower

car_data %>%
  filter(is.na(Engine.HP))

#  Using the base horsepower I can find online for these Some versions have higher horsepower, but this data set does not give us the attributes needed to identify these.

car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Model S' & Year == 2014),5] = 302
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Model S' & Year == 2015),5] = 329
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Model S' & Year == 2016),5] = 315
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='500e' & Year == 2015),5] = 111
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='500e' & Year == 2016),5] = 111
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='500e' & Year == 2017),5] = 111
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Continental' & Year == 2017),5] =305
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Escape' & Year == 2017),5] = 168
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Fit EV' & Year == 2013),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Fit EV' & Year == 2014),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Focus' & Year == 2015),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Focus' & Year == 2016),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Focus' & Year == 2017),5] = 123
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Freestar' & Year == 2005),5] = 193
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='i-MiEV' & Year == 2014),5] = 66
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Impala' & Year == 2015),5] = 195
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Impala' & Year == 2016),5] = 196
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Impala' & Year == 2017),5] = 197
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Leaf' & Year == 2014),5] = 107
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Leaf' & Year == 2015),5] = 107
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Leaf' & Year == 2016),5] = 107
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='M-Class' & Year == 2015),5] = 302
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='MKZ' & Year == 2017),5] = 240
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='RAV4 EV' & Year == 2013),5] = 154
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='RAV4 EV' & Year == 2014),5] = 154
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Soul EV' & Year == 2015),5] = 109
car_data[(is.na(car_data$Engine.HP) & car_data$Model=='Soul EV' & Year == 2016),5] = 109




# Double Check NA Values - Looks good!
gg_miss_var(car_data)
sapply(car_data, function(x) sum(is.na(x)))

```

```{r}
# Unnown Transmission Types

car_data %>%
  filter(Transmission.Type == 'UNKNOWN')

# Based off of other RAM 150s
car_data[(car_data$Transmission.Type == 'UNKNOWN' & car_data$Model=='RAM 150'),7] = 'MANUAL'

# Based off of online research
car_data[(car_data$Transmission.Type == 'UNKNOWN' & car_data$Model=='Achieva'),7] = 'AUTOMATIC'

# Other 3 models with unknown transmissions don't have enough identifiable info to fix, so we'll drop these 9 rows.

car_data = car_data %>%
  filter(Transmission.Type != 'UNKNOWN')

# Refactor transmission type to exclude UNKNOWN
car_data$Transmission.Type = as.character(car_data$Transmission.Type)
car_data$Transmission.Type = as.factor(car_data$Transmission.Type)


```

```{r}
summary(car_data)


```

```{r}


# Seeing odd values for max HP, Engine Cylinders,MPG, and MSRP

# HP and MSRP looks correct
car_data %>%
  filter(Engine.HP > 700)

# Cylinders looks correct based on car and HP
car_data %>%
  filter(Engine.Cylinders > 10)

# Highway MPG for this item is false - manual fix based on online research
car_data %>%
  filter(highway.MPG > 300)

car_data[car_data$highway.MPG == 354,13] = 32

# All high city mpgs are electric, as expected
car_data %>%
  filter(city.mpg > 100)


# N/A Strings in Market Category

# These N/A Values range over a wide variety of car types. We'll choose to keep these values in the data set unless we need to create a model using market category

head(
  car_data %>%
    filter(Market.Category == 'N/A')
    )
```



```{r}
# Added this section for Further data cleaning.
# Thoughts:
#   1. We should remove all vehicles above $500,000. The data set of car beyond that point is very small. 
#   2. We should eliminate duplicates. The data is filled with a lot of duplicate records.
#   3. After this, we'll be left with what looks like duplicates but they won't. the records remaining will have some matches except on the MSRP column. We should average those and keep one with the mean MSRP.

```





# Training / Test Split

```{r}
set.seed(1234)
index = sample(1:dim(car_data)[1],9525,replace=F)

train = car_data[index,]
test_validate = car_data[-index,]
test_index = sample(1:dim(test_validate)[1],1190,replace=F)

train = car_data[index,]
test = test_validate[test_index,]
validate = test_validate[-test_index,]

head(train)
head(test)
head(validate)
```


# EDA


```{r}

head(train)
pairs(train[,c(16,3,5,6,9,13,14,15)])

```


**Initial Findings**

  * City and highway mpg are highly correlated.
  * Rayon - I agree with this. 
 
  * Seeing some correlation between HP, Cylinders, and MPG (highway and city) as well
  * Rayon - I agree with this.
 
```{r}
pairs(train[,c(15,5,6,13,14)])
```

**Findings**
 
  * We'll need to choose between HP vs Cylinders and Highway mpg vs City mpg
 
```{r}
pairs(train[,c(16,5,6)])

pairs(train[,c(16,13,14)])

# highway mpg is statistically significant when used as a predictor for popularity (did you mean MSRP?) while city mpg is not. We'll go with highway mpg
summary(lm(MSRP ~ highway.MPG, data = train))[4]
summary(lm(MSRP ~ city.mpg, data = train))[4]

# Both are statistically significant - next we'll test to see how they hold up when paired with highway mpg
summary(lm(MSRP ~ Engine.HP, data = train))[4]
summary(lm(MSRP ~ Engine.Cylinders, data = train))[4]

pairs(train[,c(16,5,6,13)])
summary(lm(Popularity ~ Engine.HP + highway.MPG, data = train))[4]
summary(lm(Popularity ~ Engine.Cylinders + highway.MPG, data = train))[4]

# Seeing some collinearity between mpg, horsepower, and cylinders. We'll move forward with caution using highway mpg and cylinders, understanding that we may ultimately need to decide between the two.

head(train)

#Numerical
pairs(train[,c(16,6,9,13,15)])

#Seeing some correlation with MSRP vs Fuel Type and MSRP vs Make, and MSRP vs Year
pairs(train[,c(16,1,3,4)])

pairs(train[,c(16,7:11)])
```
## Importance of Popularity:

#### Popularity is broken down by make - not by model. Also, no two makes have the same popularity. This means that popularity has a 1-1 relationship with Make, so we only need one of them

```{r}
car_data %>%
  select(Make, Model, Popularity) %>%
  group_by(Make) %>%
  summarize(count = n(), max = max(Popularity), min = min(Popularity)) %>%
  arrange(max)
```

#### Popularity vs MSRP

```{r}

# Seeing a slight negative trend between popularity score and MSRP
train %>%
  ggplot(aes(x = Popularity, y = MSRP)) +
  geom_point() +
  geom_smooth(method = 'lm', color = 'red')

summary(lm(MSRP~Popularity, data = train))
```

<br>

**Findings**

  * There exists a negative correlation between popularity score and MSRP
   
    * This may indicate that more affordable cars are more popular
   
  * The relationship between popularity and MRSP is statistically significant
  
  
  
# Model Creation Work




## Choosing potential predictors
```{r}

# Checking for multicolinnearity - Not including Make, since it is dependent on popularity

head(train[,c(3,4,6,7,8,9,10,11,13,16)])

library(car)

# All VIF is under 10 (though highway MPG is close), so we'll move forward with caution
model_attributes = train[,c(3,4,6:11,13,15,16)]
full.model<-lm(MSRP~.,data=model_attributes)
vif(full.model)[,3]^2
```
# Should we removing NA from Market Categories? Probably not.
```{r}
train %>%
  filter(Market.Category != 'N/A')
test %>%
  filter(Market.Category != 'N/A')
validate %>%
  filter(Market.Category != 'N/A')

# Removing them takes out A LOT of rows. Maybe best to just not include this attribute...
```

# Predict.Regsubsets fxn

```{r}
predict.regsubsets =function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
```

# Variable Selection (Not including market categories)

## Ely Note: This seems wild. I'm clearly doing something wrong here

```{r}
library(leaps)
reg.fwd = regsubsets(MSRP~.,data=train[,c(3,4,6:9,11,13,15,16)],method="forward",nvmax=8)

data


testASE<-c()

for (i in 1:8){
  predictions<-predict.regsubsets(object=reg.fwd,newdata=test,id=i) 
  testASE[i]<-mean((test$MSRP-predictions)^2)
}

par(mfrow=c(1,1))
plot(1:8,testASE,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(0,3000000000))
index<-which(testASE==min(testASE))
points(index,testASE[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss
lines(1:8,rss/9525,lty=3,col="blue")   #Dividing by 192 since ASE=RSS/sample size NEED TO ADJUST SAMPLE SIZE DIVISION!!!!
```



# Lasso Attempt

```{r}
library(glmnet)
#Formatting data for GLM net
x=model.matrix(MSRP~.,train[,c(3,4,6:9,11,13,15,16)])[,-1]
y=train$MSRP

xtest=model.matrix(MSRP~.,test[,c(3,4,6:9,11,13,15,16)])[,-1]
ytest=test$MSRP



x=model.matrix(MSRP~.,train)[,-1]
y=train$MSRP

xtest=model.matrix(MSRP~.,test)[,-1]
ytest=test$MSRP


grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)
bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
lasso.pred=predict (lasso.mod ,s=bestlambda ,newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO


coef(lasso.mod,s=bestlambda)
```


# Here's the model LASSO picked - obviously some major collinearities here. Need to work through!
```{r}
model = lm(MSRP~Make + Model + Year + Engine.Fuel.Type + Engine.Cylinders+Transmission.Type+Driven_Wheels+Number.of.Doors+Market.Category+Vehicle.Size+Vehicle.Style+highway.MPG+city.mpg+Popularity, data = train)

summary(model)

par(mfrow=c(2,2))
plot(model)


# Slight adjustment - removed obvious collinearities. Still need work

model = lm(MSRP~Model + Year + Engine.Fuel.Type + Engine.Cylinders+Transmission.Type+Driven_Wheels+Number.of.Doors+Market.Category+Vehicle.Style+highway.MPG+Popularity, data = train)

summary(model)

par(mfrow=c(2,2))
plot(model)


```

# Initial Model - WIP

### I chose not to include engine fuel type, since the only significant variable in that set was flex-fuel (premium unleaded required/E85) 

```{r}
model = lm(MSRP~Year+Transmission.Type+Driven_Wheels+Number.of.Doors+Vehicle.Size+Popularity, data = train)

summary(model)

par(mfrow=c(2,2))
plot(model)
```


### This model still is not great - I wonder if we should adjust our response variable ...
### Seeing evidence of violations of normality and equal variance



```{r}
# Logged the response variable but seeing a sharp band in teh Residuals. The qqplot looks a lot better. However, we can see that there are some outliers in the data. Maybe we should some more cleaning. Left a comment up above.


logmodel = lm(log(MSRP)~Year+Transmission.Type+Driven_Wheels+Number.of.Doors+Vehicle.Size+Popularity, data = train)

summary(logmodel)

par(mfrow=c(2,2))
plot(logmodel)

```







